{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea Recuperativa: Implementación de Black Box\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.misc import derivative\n",
    "from scipy.stats import norm,multivariate_normal\n",
    "from scipy.integrate import quad,dblquad\n",
    "from scipy.special import gamma\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Transformación a la matriz de diseño\n",
    "def phi_pol(xi,p):\n",
    "    result = []\n",
    "    for i in range(0,p+1):\n",
    "        result.append(np.power(xi,i))\n",
    "    return np.array(result)\n",
    "def X_pol(X,p):\n",
    "    result = phi_pol(X[0],p)\n",
    "    for i in range(1,len(X)):\n",
    "        ph = phi_pol(X[i],p)\n",
    "        result = np.vstack((result,ph))\n",
    "    return result\n",
    "\n",
    "## Transformación R^3 a matriz definida positiva:\n",
    "\n",
    "def ftransf(x):\n",
    "    a = x[0]\n",
    "    b = x[1]\n",
    "    c = x[2]\n",
    "    A = np.array([[a,b],[b,c]])\n",
    "    return np.dot(A,A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La Matriz dada por \n",
    "$$ A = \\begin{pmatrix}\n",
    "a & b \\\\\n",
    "b & c\n",
    "\\end{pmatrix}$$ \n",
    "\n",
    "es simétrica. Además, la matriz $A \\cdot A$ es siempre definida positiva. En efecto:\n",
    "$$ x^T A^2 x = x^T A\\cdot A x = x^T A^TA x = (Ax)^T(Ax) = \\| Ax\\|^2 \\geq 0 $$\n",
    "Por lo tanto, con la funcion ftransf podemos converitr 3 elementos arbitrarios en una matriz simétrica semidefinida positiva, que es necesario para construir normales multivariadas. De esta forma, podemos derivar con respecto a un vector en R^3 y no con respecto a una matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ad8a5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def data(file):\n",
    "    data = np.genfromtxt(file,delimiter =\",\",dtype = None)\n",
    "    X = np.array(data[1:,0],dtype = float)\n",
    "    Y = np.array(data[1:,1],dtype=float)\n",
    "    X = (X-X.mean())/(X.std())\n",
    "    Y = (Y-Y.mean())/(Y.std())\n",
    "    return (X,Y)\n",
    "\n",
    "file = \"datos.txt\"\n",
    "\n",
    "X,Y= data(file)\n",
    "plt.plot(X,Y,\"o\")\n",
    "\n",
    "# Variables Globales\n",
    "\n",
    "n=X.shape[0]\n",
    "a0, b0 =2,2\n",
    "Phi = X_pol(X,1)\n",
    "betainv= 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Lineal Bayesiana\n",
    "\n",
    "Utilizamos la verisón de regresión bayesiana predictiva disponible en el libro de Bishop : Pattern Recognition and Machine Learning, desde la página 486. En ella, la distribución conjunta se obtiene:\n",
    "\n",
    "$$ p(t,w,\\alpha) = p(t|w)p(w|\\alpha)p(\\alpha) $$\n",
    "\n",
    "donde:\n",
    "1. $p(t|w) = \\Pi_{n=1}^N \\mathcal{N}(t_n | w^T \\phi(x_n),\\beta^{-1}$)\n",
    "2. $p(w|\\alpha) = \\mathcal{N}(w|0,\\alpha^{-1}I)$\n",
    "3. $p(\\alpha) = \\mbox{Gamma}(\\alpha|a_0,b_0)$\n",
    "\n",
    "En las definiciones $\\phi$ es una transformación de los datos constituida por un kernel (en nuestro caso, el polinomio lineal). Los valores $\\beta$,$a_0,b_0$ son parámetros del modelo.\n",
    "\n",
    "La aproximación variacional utilizada en este caso es de la forma:\n",
    "$$q(w,\\alpha) = q(w)q(\\alpha) $$\n",
    "donde:\n",
    "1. $q(w) = \\mathcal{N}(w | m_N,S_n)$\n",
    "2. $q(\\alpha) = \\mbox{Gamma}(\\alpha| a_N,b_N)$\n",
    "\n",
    "donde la elección de los parámetros $m_N,S_n,a_N$ y $b_N$ es el foco del problema de Black Box, puesto que em mecanismo Mean Field presume desarrollos complejos para encontrar los valores óptimos.\n",
    "\n",
    "Adicionalmente, hemos implementado el algoritmo de Metropolis- Hastings para generar muestras de la distirbución $Q$, necesarias en el modelo de Black Box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funciones de Distribución\n",
    "\n",
    "def P(t,w,alpha,argsP,extraP): #Ditribución Conjunta para Modelo de Regresion Lineal Bayesiano\n",
    "\n",
    "    #Parametros generales\n",
    "    X= extraP[0]\n",
    "    n = X.shape[0]\n",
    "    wdim = w.shape[0]\n",
    "    Phi= X_pol(X,1)\n",
    "    \n",
    "    ## Hyper parametros de P\n",
    "    betainv = argsP[0]\n",
    "    a = agrsP[1]\n",
    "    b = argsP[2]\n",
    "    \n",
    "    # Distribuciones marginales\n",
    "    \n",
    "    P1 = lambda x,y :np.prod( [ multivariate_normal.pdf(x[i],mean=np.dot(y,Phi[i,:]) ,cov= betainv ) for i in range(n)])\n",
    "    P2 = lambda x,y : multivariate_normal.pdf(w,mean= np.zeros((wdim,)),cov=(1/y)*np.eye(wdim))\n",
    "    P3 = lambda x : Gamma(x,a,b)\n",
    "                                              \n",
    "    return P1(t,w)*P2(w,alpha)*P3(alpha)\n",
    "\n",
    "def Q(X,argsQ):\n",
    "    w =(X[0],X[1])\n",
    "    alpha = X[2]\n",
    "    \n",
    "    a=argsQ[0]\n",
    "    b=argsQ[1]\n",
    "    mean = argsQ[2]\n",
    "    cov = argsQ[3]\n",
    "    \n",
    "    Q1 = lambda x : multivariate_normal.pdf(x,mean = mean ,cov = cov )\n",
    "    Q2 = lambda x : Gamma(x,a,b)\n",
    "    \n",
    "    return Q1(w)*Q2(alpha)\n",
    "\n",
    "def Gamma(x,a,b):\n",
    "    return (b**a)*(x**(a-1))*np.exp(-b*x)/gamma(a)\n",
    "\n",
    "## Generar Muestras Aleatorias\n",
    "\n",
    "def MetropolisHastings(Ps,argsPs,x0,M):\n",
    "    samples = []\n",
    "    xs = x0\n",
    "    for s in range(3*M):\n",
    "        x = xs\n",
    "        xp = multivariate_normal.rvs(mean=x, cov=covM, size=1)\n",
    "        alpha = (Ps(xp,argsPs)/(Ps(x,argsPs)))\n",
    "        r = min(1,alpha)\n",
    "        u = np.random.uniform(0,1)\n",
    "        if u<r:\n",
    "            xs = xp\n",
    "        else:\n",
    "            xs = xs\n",
    "        samples.append(xs)\n",
    "    samples = samples[2*M:] # Solo para asegurarme jeje\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q inicial\n",
    "mean = np.array([0,0])\n",
    "cov = 3*np.eye(2)\n",
    "argsQ = [a0,b0,mean,cov]\n",
    "covM = 3*np.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testeo Metropolis Hastings\n",
    "M = 100\n",
    "x0 = (1,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = MetropolisHastings(Q,argsQ,x0,M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Black Box\n",
    "\n",
    "A continuación describimos el algoritmo 2 de Black-Box, descrito en el paper \"Black Box Variational Inference\", de R. Ranganath, S. Gerrish y D.M. Blei :\n",
    "\n",
    "1. **Input** Datos $x$, distribución conjunta $p$, familia variacional $q$.\n",
    "2. **Inicializar** $\\lambda_{1:n}$ aleatoriamente, t=1.\n",
    "3. **Repetir**:\n",
    "   * Samplear $S$ muestras de la aprovimación variacional\n",
    "   * for $s=1$ to $S$ do:\n",
    "       $$z[s]\\sim q$$\n",
    "     end for\n",
    "   * for $i=1$ to $n$ do:\n",
    "          - for $s=1$ to $S$ do:\n",
    "       $$ f_i[s] =\\nabla_{\\lambda_i} \\log q_i (z[s]|\\lambda_i)(\\log p_i(x,z[s])-\\log q_i(z[s]|\\lambda_i)$$\n",
    "       $$ h_i[s]= \\nabla_{\\lambda_i} \\log q_i (z[s]|\\lambda_i)$$\n",
    "       end for\n",
    "       $$\\hat{a_i}^* = \\dfrac{\\sum_{d=1}^{n_i} Cov(f_i^d,h_i^d) }{\\sum_{d=1}^{n_i} Var(h_i^d)} $$\n",
    "   end for\n",
    "   $$\\rho = t\\mbox{ ésimo de una secuencia Robbins Monro}$$\n",
    "   $$\\lambda = \\lambda+\\rho \\nabla_{\\lambda} \\mathcal{L} $$\n",
    "   $$ t = t+1 $$\n",
    "   \n",
    "## Explicación del Modelo\n",
    "\n",
    "El algoritmo anterior surge debido a un intento de encontrar aquellos parametros para la distribuición $Q$ que nos permiten maximizar el ELBO. Debido a que buscamos evitar desarrollos teóricos dependientes del modelo que pueden lelgar a ser muy complejos, se utiliza un acercamiento de método de descenso estocástico (optimización estocástica).\n",
    "\n",
    "Debido a que queremos optimizar el ELBO, el enfoque estándar es tomar su gradiente e igualar a cero. Debido a la dificultad que esto implica, la alternativa utilizada es la siguiente: aproximar un estimador insesgado del gradiente del ELBO, que pueda ser calculado a partir de muestras de la distribución $Q$. El gradiente teórico está dado por la Ecuación 2 del paper:\n",
    "\n",
    "$$ \\nabla_{\\lambda} \\mathcal{L} = E_q[ \\nabla_{\\lambda} \\log q(z | \\lambda)( \\log p(x,z)- \\log q(z | \\lambda)$$\n",
    "\n",
    "Debido a que, como se sabe se cursos de estadística anteriores, por la ley de los grandes Números sabemos que la Esperanza matemática puede ser estimada por un promedio de evaluaciones en las muestras. Además, el promedio es por naturaleza un estimador insesgado  y así el Teorema de Rao Blackwell puede ser utilizado sin problemas para el control de varianza, que veremos después. Luego, nuestra estimación está dada por la ecuación (3) del paper:\n",
    "\n",
    "$$\\nabla_{\\lambda} \\mathcal{L} = \\dfrac{1}{S} \\sum_{s=1}^S \\log q(z_s | \\lambda)( \\log p(x,z_s)- \\log q(z_s| \\lambda)$$\n",
    "\n",
    "donde $z_s \\sim q(z|\\lambda)$.\n",
    "\n",
    "Debido a que es un algoritmo de descenso de gradiente esctócastico, es necesario controlar la varianza del ruido de dicho gradiente, pues esta puede ser demasiado grande como para que el algoritmo sea útil. Para ello, se introducen dos métodos de reducción y control de la marianza. El primero, la **Rao-Blackwellization** y el segundo **Control Variates**.\n",
    "\n",
    "En el caso del primero, **Rao Blackwellization** remplaza una función de dos variables $J(X,Y)$ por otra de una variable, que se construye al condicionar $J$ por $X$. De esa forma, se mantienen las esperanzas pero por el Teorema de Rao Blackwell, se disminuye la varianza. En efecto: si llamamos $M(X) = E[J(X,Y)|X]$, se tendrá por propiedades conocidas de la esperanza que:\n",
    "$$ E[M(X)] = E[E[J(X,Y)|X]] = E[E[J(X,Y)]] = E[J(X,Y)]$$\n",
    "Y por lo tanto se conserva la esperanza. Sin embargo, se puede demostrar que (Teorema de Rao- Blackwell):\n",
    "$$ Var(M(X)) = Var(J(X,Y)) - E( (J(X,Y)-M(X))^2)$$.\n",
    "\n",
    "\n",
    "Debido a que el  término restado es positivo, se tiene que la varianza de nuestro nuevo funcional es menor que la original. Para utilizar lo anterior, en vez de calcular gradientes contra $\\lambda$, lo calculamos contra $\\lambda_i$, donde $i$ representa la $i$-esima distribución que conforma a $Q$ de la forma (ecuación 4):\n",
    "\n",
    "$$ q(z|\\lambda ) = \\Pi_{i=1}^n q_i(z_i |\\lambda_i)$$\n",
    "\n",
    "Y así, nuestra estimación queda como en la ecuación 6 del paper:\n",
    "\n",
    "$$\\nabla_{\\lambda_i} \\mathcal{L} = \\dfrac{1}{S} \\sum_{s=1}^S \\log q_i(z_s | \\lambda_i)( \\log p_i(x,z_s)- \\log q_i(z_s| \\lambda_i)$$\n",
    "constituye un estimador de Rao-Blackwell, que reduce la varianza. Es necesario ser cuidadoso con la elección de $p_i(x,z_i)$, que corresponde a las distribución conjunta de quellas variables latentes $z_i$ que son necesarias para el cálculo de $p_i$.\n",
    "\n",
    "Finalmente, utilizamos **Control Variates**. Para ello, se consideran funciones $h$ integrales (primer momento finito) y se construye una función $\\bar f$ de manera que su esperanza sea igual a otra función $f$, pero con menor varianza. Y así, se construye:\n",
    "$$ \\bar f(z) = f(z) - a(h(z)-E(h(z))$$\n",
    "donde la elección de $a$ y $h$ es la escrita en el algoritmo 2. Debido a que:\n",
    "$$ Var(\\bar f) = Var(f) +a^2Var(h) - 2aCov(f,h)$$\n",
    "queda a simple vista que si la covarianza entre ambas funciones es alta, entonces la varianza de $\\bar f$ disminuye (las ecuaciones anteriores corresponden a la ecuación 7 del paper, y conocimiento general de varianzas). \n",
    "De esta forma, se busca una función $h$ que este altamente correlacionada con $f$. Adicionalmente, el valor óptimo para $a$ está dado al derivar la expresión anterior e igualarla a 0. Las ecuaciones para esto son (ecuación 8 del paper):\n",
    "\n",
    "$$f_i(z) = \\nabla_{\\lambda_i} \\log q(z|\\lambda_i)(\\log p(x,z)-\\log q(z|\\lambda_i))$$\n",
    "$$ h_i(z) = \\log q(z|\\lambda_i)$$\n",
    "\n",
    "Podemos ver que están muy correlacionados debido al término $\\log q(z|\\lambda_i)$ en común.\n",
    "\n",
    "Los supraindices $d$ de nuestro algoritmo indican una discretización en los cálculos a través de estimación empírica usando sampleo de variables aleatorias, pero manteniendo las propiedades deseasdas. Se puede ver en el algoritmo los cálculos para el caso discreto.\n",
    "\n",
    "Con lo anterior, se construyen variables aleatorias que mantienen la esperanza, y por lo tanto mantienen nuestro estimador del gradiente del ELBO, pero que poseen menor varianza que el original, de manera que nuestro gradiente estocástico posee menor ruido y por lo tanto presenta mayor utilizad para ser utilizado en un algoritmo de descenso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación\n",
    "\n",
    "Debido a que es necesario obtener gradientes, hemos utilizado la función \"DerivadaParcial\", obtenida de: https://stackoverflow.com/questions/20708038/scipy-misc-derivative-for-mutiple-argument-function. Adicionalmente, hemos implementado los logaritmos de las funciones $q_i$ y $p_i$, puesto que son necesarios para los calculos de Black Box. \n",
    "\n",
    "Vale la pena destacar que para nuestro caso hemos creado una transformación que convierte un vector en $\\mathbb{R}^3$ en una matriz simétrica semidefinida positiva, necesaria para construir la matriz de covarianzas. Con dicha transformación (que además es sobreyectiva), es posible calcular gradientes sin la necesidad de derivadas más generales sobre objetos como matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DerivadaParcial(func, var=0, point=[]):\n",
    "    args = point[:]\n",
    "    def wraps(x):\n",
    "        args[var] = x\n",
    "        return func(*args)\n",
    "    return derivative(wraps, point[var], dx = 1e-6)\n",
    "\n",
    "\n",
    "# Funciones log\n",
    "\n",
    "#Q\n",
    "def logq1(x0,x1,mu1,mu2,a,b,c): #Normal multivariada: Estimamos 5 parámetros\n",
    "    mean = np.array([mu1,mu2])\n",
    "    cov = ftransf((a,b,c))\n",
    "    return np.log(multivariate_normal.pdf((x0,x1),mean = mean ,cov = cov ))\n",
    "def logq2(x,a,b):\n",
    "    return np.log(Gamma(x,a,b))\n",
    "#P \n",
    "def logp1(data,y):\n",
    "    return (np.prod([norm.pdf(data[i],loc=np.dot(y,Phi[i,:]) ,scale= betainv ) for i in range(n)]))\n",
    "def logp2(x,alpha):\n",
    "    w = np.array(x)\n",
    "    wdim = w.shape[0]\n",
    "    return np.log(multivariate_normal.pdf(w,mean= np.zeros((wdim,)),cov=(1/(np.abs(alpha)))*np.eye(wdim)))\n",
    "def logp3(x,a,b):\n",
    "    return np.log(Gamma(x,a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, implementamos el calculo de gradientes. Habría sido posible implementar esto de manera más limpia, pero debido a la falta de tiempo preferí hacerlo a la bruta (no me funcionaron intentos más generales). Pido disculpas. \n",
    "\n",
    "Las funciones básicamente hacen lo siquiente:\n",
    "\n",
    "1. gradlogq1 : derivada parcial del logaritmo de q1 en la coordenada i\n",
    "2. gradlogq2: derivada parcial del logaritmo de q2 en la coordenada i\n",
    "3. logpq1: Funcion a derivar necesaria para calcular $f_1[s]$ \n",
    "4. logpq2: Funcion a derivar necesaria para calcular $f_2[s]$ \n",
    "5. Gradiente: Báscicamente, se le introduce una función y es capaz de calcular gradientes en algun punto (entregando la versión vectorial.\n",
    "\n",
    "Las variables point1 y point2 se refieren a:\n",
    "1. point1: muestra generada a partir de Q\n",
    "2. point2: Esta representa las coordenadas necesarias para evaluar los hiperparametros que estan siendo estimados. Este punto juega el rol de un $\\lambda_i$.\n",
    "\n",
    "Y finalmente:\n",
    "1. gradlogpq1: derivada parcial de los terminos necesarios para calcular $f_1[s]$ en la componente i\n",
    "2. gradlogpq2: derivada parcial de los terminos necesarios para calcular $f_2[s]$ en la componente i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradientes: Necesarios para la ejecución \n",
    "\n",
    "def gradlogq1(point1,point2,i):\n",
    "    logq1xD = lambda mu1,mu2,a,b,c : logq1(point1[0],point1[1],mu1,mu2,a,b,c)\n",
    "    gradlogq1xD = DerivadaParcial(logq1xD,var=i,point=point2)\n",
    "    return gradlogq1xD\n",
    "def gradlogq2(point1,point2,i):\n",
    "    logq2xD = lambda a,b : logq2(point1,a,b)\n",
    "    gradlogq2xD = DerivadaParcial(logq2xD,var=i,point=point2)\n",
    "    return gradlogq2xD [0]\n",
    "\n",
    "\n",
    "def Gradiente(grad,point1,point2,n):\n",
    "    gradiente = []\n",
    "    for i in range(n):\n",
    "        gradiente.append(grad(point1,point2,i))\n",
    "    return np.array(gradiente)\n",
    "\n",
    "## Otro gradiente relevante\n",
    "\n",
    "#logp1(Y,point1[:2])\n",
    "# necesitamos usar p1 y p2, pues ellos dependen de w y alpha, que son lo que estamos estimando. Point1 viene siendo una muestra\n",
    "def logpq1(mu1,mu2,a,b,c,point1):\n",
    "    return logq1(point1[0],point1[1],mu1,mu2,a,b,c)*(logp2(point1[:2],point1[2:][0])-logq1(point1[0],point1[1],mu1,mu2,a,b,c))\n",
    "def logpq2(a,b,point1):\n",
    "    return logq2(point1[2:][0],a,b)*(logp2(point1[:2],point1[2:][0])+logp3(point1[2:][0],a0,b0)-logq2(point1[2:][0],a,b))\n",
    "\n",
    "def gradlogpq1(point1,point2,i):\n",
    "    logpq1xD = lambda mu1,mu2,a,b,c : logpq1(mu1,mu2,a,b,c,point1)\n",
    "    gradlogpq1xD = DerivadaParcial(logpq1xD,var=i,point=point2)\n",
    "    return gradlogpq1xD\n",
    "def gradlogpq2(point1,point2,i):\n",
    "    logpq2xD = lambda a,b : logpq2(a,b,point1)\n",
    "    gradlogq2xD = DerivadaParcial(logpq2xD,var=i,point=point2)\n",
    "    return gradlogq2xD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionalmente, implementamos una sucesión cuadrado convergente, pero linealmente divergente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RobbinsMonro(a,n):\n",
    "    return (a/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, implementamos el método Black Box para inferencia variacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VariationalBlackBox(Qs,argsQs,S,x0):\n",
    "    #Parametros de iteración\n",
    "    n = 7\n",
    "    t = 1\n",
    "    rho = lambda t : RobbinsMonro(2,t)\n",
    "    error = 2\n",
    "    errores = []\n",
    "    # Parte 1 : Inicializacion\n",
    "    \n",
    "    lambdinit = 50*np.random.rand(7)\n",
    "    lambdactual = lambdinit\n",
    "    \n",
    "    #Parte 2: Iteración\n",
    "     \n",
    "    while error > 0.01:\n",
    "        \n",
    "        lambdviejo = lambdactual\n",
    "        \n",
    "        point21 = lambdviejo[:5]\n",
    "        point22 = lambdviejo[5:]\n",
    "        \n",
    " #Actualización de parámetros\n",
    "\n",
    "        mu1 = lambdactual[0]\n",
    "        mu2 = lambdactual[1]\n",
    "        a = lambdactual[2]\n",
    "        b =lambdactual[3]\n",
    "        c =lambdactual[4]\n",
    "        aN =lambdactual[5]\n",
    "        bN=lambdactual[6]\n",
    "        argsQs = [aN,bN,np.array([mu1,mu2]),ftransf((a,b,c))]\n",
    "        \n",
    "        # Draw Samples\n",
    "        samples = MetropolisHastings(Qs,argsQs,x0,S)\n",
    "        \n",
    "        # Arreglos a ir guardando\n",
    "        \n",
    "        h1 = []\n",
    "        h2 = []\n",
    "        f1 = []\n",
    "        f2 = []\n",
    "        \n",
    "        ## para n = 1 # Perdon por lo \"poco inteligente\n",
    "        \n",
    "        for i in range(S):\n",
    "            his1 = Gradiente(gradlogq1,samples[i],point21,5)\n",
    "            his2 = Gradiente(gradlogq2,samples[i],point22,2)\n",
    "            h1.append(his1)\n",
    "            h2.append(his2)\n",
    "            \n",
    "            fis1 = Gradiente(gradlogpq1,samples[i],point21,5)\n",
    "            fis2 = Gradiente(gradlogpq2,samples[i],point22,2)\n",
    "            f1.append(fis1)\n",
    "            f2.append(fis2)\n",
    "        \n",
    "        h1 = np.array(h1).T\n",
    "        h2 = np.array(h2).T\n",
    "        f1 = np.array(f1).T  \n",
    "        f2 = np.array(f2).T\n",
    "            \n",
    "#        x1 = f1.sum(axis=1)/S\n",
    "#        y1 = h1.sum(axis=1)/S\n",
    "#        X1 = np.vstack((x1,y1))\n",
    "#        Y1 = np.vstack((y1,y1))\n",
    "        \n",
    " #       x2 = f1.sum(axis=1)/S\n",
    " #       y2 = h1.sum(axis=1)/S\n",
    " #       X2 = np.vstack((x2,y2))\n",
    " #       Y2 = np.vstack((y2,y2))\n",
    "    \n",
    "        a1 =   np.sum( [np.cov(f1[i,:],h1[i,:])[0][1] for i in range(5)]) / np.sum( [np.cov(h1[i,:],h1[i,:])[0][1] for i in range(5)])\n",
    "        a2 =   np.sum( [np.cov(f2[i,:],h2[i,:])[0][1] for i in range(2)]) / np.sum( [np.cov(h2[i,:],h2[i,:])[0][1] for i in range(2)])\n",
    "     \n",
    "       # a1 = np.sum(np.cov(X1))/ np.sum(np.cov(Y1))\n",
    "      #  a2 = np.sum(np.cov(X2))/ np.sum(np.cov(Y2)) \n",
    "        \n",
    "        gradL1 = GradLi(f1,h1,a1,S)\n",
    "        gradL2 = GradLi(f2,h2,a2,S)\n",
    "        \n",
    "        gradL = np.hstack((gradL1,gradL2))\n",
    "        lambdactual = lambdactual+rho(t)*gradL\n",
    "        \n",
    "        \n",
    "        t=t+1\n",
    "        print (t)\n",
    "        \n",
    "        error = np.linalg.norm(lambdviejo-lambdactual)\n",
    "        errores.append(error)\n",
    "    return (lambdactual,errores)\n",
    "\n",
    "def GradLi(fi,hi,ai,S):\n",
    "    vec = 0\n",
    "    for i in range(S):\n",
    "        vec += fi[:,i]-ai*hi[:,i]\n",
    "    return vec/S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in power\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ -35.98475568,   74.25969298,  111.05755314,  -29.68036719,\n",
       "          36.54633048,           nan,           nan]), [nan])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = 20\n",
    "lambd = VariationalBlackBox(Q,argsQ,S,x0)\n",
    "lambd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, no logré calibrar el modelo para entregar valores que no dividieran por cero. Sin embargo, si se analiza el codigo entregado se puede concluir que es una implementación bien hecha del problema.\n",
    "\n",
    "Quiero aclarar la salvedad de que quité $\\log p_1$ del calculo puesto que se me iba a $-\\infty$ muy rapido, debido a la naturaleza de los datos. Si huibiese podido calibrar mejor, no habría habido problemas. Pido piedad en la corrección plis :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gráficos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.5,  2.5],\n",
       "       [ 2.5,  2.5]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
